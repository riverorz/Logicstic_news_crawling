{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import chardet\n",
    "import time\n",
    "# coding=utf-8\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "hannanum=Hannanum()\n",
    "twitter=Twitter()\n",
    "kkma = Kkma()\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('navernews.db')\n",
    "df = pd.read_pickle('./loginews.pkl')\n",
    "df = pd.read_sql(\"SELECT * FROM logistic\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'].astype('str')\n",
    "df['data'] = pd.to_datetime(df['data'], format = \"%Y-%m-%d\")\n",
    "df_c =df['content'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(df_c)+1):\n",
    "    a = df_c[i].split()\n",
    "    df_c[i] = \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content']  = df_c\n",
    "df[df['content'] =='']\n",
    "df = df[df['content'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'].iloc[0]\n",
    "print(kkma.pos(df['content'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거합니다.\n",
    "def text_cleaning(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글의 정규표현식을 나타냅니다.\n",
    "    result = hangul.sub('', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ko_text'] = df['content'].apply(lambda x: text_cleaning(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['ko_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kkma.pos(df['ko_text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 한국어 약식 불용어사전 예시 파일입니다. 출처 - (https://www.ranks.nl/stopwords/korean)\n",
    "korean_stopwords_path = \"./korean_stopwords.txt\"\n",
    "with open(korean_stopwords_path, encoding='utf8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "def get_nouns(x):\n",
    "    nouns_tagger =Okt()\n",
    "    nouns = nouns_tagger.nouns(x)\n",
    "    \n",
    "    # 한글자 키워드를 제거합니다.\n",
    "    nouns = [noun for noun in nouns if len(noun) >1]\n",
    "    \n",
    "    # 불용어를 제거합니다.\n",
    "    nouns = [noun for noun in nouns if noun not in stopwords]\n",
    "    \n",
    "    \n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['nouns'] = df['ko_text'].apply(lambda x : get_nouns(x))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['nouns'].iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['STD_Y'] =df['data'].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df[df['STD_Y']=='2017']\n",
    "df_2017 = df_2017[df_2017['class'] != '인사동정']\n",
    "df_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = df[df['STD_Y']=='2018']\n",
    "df_2018 = df_2018[df_2018['class'] != '인사동정']\n",
    "df_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df[df['STD_Y']=='2019']\n",
    "df_2019 = df_2019[df_2019['class'] != '인사동정']\n",
    "df_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = df[df['STD_Y']=='2020']\n",
    "df_2020 = df_2020[df_2020['class'] != '인사동정']\n",
    "df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2017['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_2020['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_2017))\n",
    "print(len(df_2018))\n",
    "print(len(df_2019))\n",
    "print(len(df_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transactions = df_2017['nouns'].tolist()\n",
    "transactions = [transaction for transaction in transactions  if transaction]# 공백 문자열을 방지합니다.\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_corpus = \"\".join(df_2017['ko_text'].tolist())\n",
    "tweet_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 명사 키워드를 추출합니다.\n",
    "nouns_tagger = Okt()\n",
    "nouns = nouns_tagger.nouns(tweet_corpus)\n",
    "count = Counter(nouns)\n",
    "\n",
    "# 한글자 키워드를 제거합니다.\n",
    "remove_char_counter = Counter({x : count[x] for x in count if len(x) >1})\n",
    "print(remove_char_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "node_df  =pd.DataFrame(remove_char_counter.items(), columns=['node', 'nodesize'])\n",
    "node_df  = node_df[node_df['nodesize'] >=50] # 시각화의 편의를 위해 ‘nodesize’ 50 이하는 제거합니다.\n",
    "node_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "line_list = df['nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "topic = 5\n",
    "keyword = 10\n",
    "texts = []\n",
    "resultList = []\n",
    "\n",
    "for line in line_list:\n",
    "    words = line\n",
    "    if words != [\"\"]:\n",
    "        tokens = [word for word in words if (len(word.split(\"/\")[0]) > 1 and word.split(\"/\")[0] not in korean_stopwords_path)]\n",
    "        texts.append(tokens)\n",
    "dictionary = corpora.Dictionary(texts)    \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=topic, id2word = dictionary, passes=10)\n",
    "for num in range(topic):\n",
    "    resultList.append(ldamodel.show_topic(num, keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resultList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
